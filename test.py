# test_correct.py
"""
æ­£ç¡®çš„æµ‹è¯•è„šæœ¬ - åŒ…å«åæ ‡å½’ä¸€åŒ–
"""
import os
import torch
import numpy as np
import json
import math
import matplotlib.pyplot as plt

# æ–¹æ³•1ï¼šå°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¡ç®—å½’ä¸€åŒ–å‚æ•°
with open('train_annotations.json', 'r') as f:
    train_data = json.load(f)

lats = [item['lat'] for item in train_data]
lons = [item['lon'] for item in train_data]

LAT_MIN, LAT_MAX = min(lats), max(lats)
LON_MIN, LON_MAX = min(lons), max(lons)

print("ğŸ“ åæ ‡å½’ä¸€åŒ–å‚æ•°:")
print(f"  çº¬åº¦: [{LAT_MIN:.3f}, {LAT_MAX:.3f}]")
print(f"  ç»åº¦: [{LON_MIN:.3f}, {LON_MAX:.3f}]")

def normalize_coords(lats, lons):
    """å½’ä¸€åŒ–åæ ‡åˆ° [0, 1]"""
    norm_lats = (lats - LAT_MIN) / (LAT_MAX - LAT_MIN)
    norm_lons = (lons - LON_MIN) / (LON_MAX - LON_MIN)
    return norm_lats, norm_lons

def denormalize_coords(norm_lats, norm_lons):
    """åå½’ä¸€åŒ–åæ ‡"""
    lats = norm_lats * (LAT_MAX - LAT_MIN) + LAT_MIN
    lons = norm_lons * (LON_MAX - LON_MIN) + LON_MIN
    return lats, lons

def haversine_distance(lat1, lon1, lat2, lon2, R=6371000.0):
    """è®¡ç®—Haversineè·ç¦»ï¼ˆç±³ï¼‰"""
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
    c = 2 * math.asin(math.sqrt(a))
    return R * c

def test_model_correctly(checkpoint_path, num_samples=50):
    """æ­£ç¡®æµ‹è¯•æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {checkpoint_path}")
    
    # å¯¼å…¥æ¨¡å‹
    from models.crossview_model import AdvancedCrossViewGeolocator
    from dataset.crossview_real_dataset import RealUniversityDataset
    from torch.utils.data import DataLoader
    
    # è®¾å¤‡
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ•°æ®é›†
    dataset = RealUniversityDataset(
        split='train',  # ç”¨è®­ç»ƒé›†æµ‹è¯•
        max_samples=num_samples
    )
    dataloader = DataLoader(dataset, batch_size=4, shuffle=False)
    
    # åŠ è½½æ¨¡å‹
    model = AdvancedCrossViewGeolocator().to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # æµ‹è¯•
    all_distances = []
    
    with torch.no_grad():
        for batch in dataloader:
            uav_imgs = batch['uav'].to(device)
            sat_imgs = batch['satellite'].to(device)
            raw_lats = batch['lat'].cpu().numpy()  # æ³¨æ„ï¼šè¿™é‡Œå·²ç»æ˜¯å½’ä¸€åŒ–çš„ï¼
            raw_lons = batch['lon'].cpu().numpy()
            
            # æ¨¡å‹é¢„æµ‹ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
            outputs = model(uav_imgs, sat_imgs)
            pred_norm_coords = outputs['fine_coords'].cpu().numpy()
            
            # åå½’ä¸€åŒ–é¢„æµ‹åæ ‡
            pred_lats, pred_lons = denormalize_coords(
                pred_norm_coords[:, 0], pred_norm_coords[:, 1]
            )
            
            # åå½’ä¸€åŒ–çœŸå®åæ ‡
            true_lats, true_lons = denormalize_coords(raw_lats, raw_lons)
            
            # è®¡ç®—çœŸå®è·ç¦»
            for i in range(len(pred_lats)):
                distance = haversine_distance(
                    pred_lats[i], pred_lons[i],
                    true_lats[i], true_lons[i]
                )
                all_distances.append(distance)
    
    # è®¡ç®—ç»Ÿè®¡
    distances = np.array(all_distances)
    
    metrics = {
        'num_samples': len(distances),
        'avg_distance_m': float(np.mean(distances)),
        'avg_distance_km': float(np.mean(distances) / 1000),
        'median_distance_m': float(np.median(distances)),
        'min_distance_m': float(np.min(distances)),
        'max_distance_m': float(np.max(distances)),
        'std_distance_m': float(np.std(distances)),
    }
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š æ­£ç¡®æµ‹è¯•ç»“æœ:")
    print(f"  æ ·æœ¬æ•°: {metrics['num_samples']}")
    print(f"  å¹³å‡è¯¯å·®: {metrics['avg_distance_m']:.1f}ç±³ ({metrics['avg_distance_km']:.2f}å…¬é‡Œ)")
    print(f"  ä¸­ä½æ•°è¯¯å·®: {metrics['median_distance_m']:.1f}ç±³")
    print(f"  æœ€å°è¯¯å·®: {metrics['min_distance_m']:.1f}ç±³")
    print(f"  æœ€å¤§è¯¯å·®: {metrics['max_distance_m']:.1f}ç±³")
    print(f"  æ ‡å‡†å·®: {metrics['std_distance_m']:.1f}ç±³")
    
    # è®¡ç®—ç²¾åº¦
    accuracy_10m = np.mean(distances < 10)
    accuracy_50m = np.mean(distances < 50)
    accuracy_100m = np.mean(distances < 100)
    
    print(f"\nğŸ¯ ç²¾åº¦æŒ‡æ ‡:")
    print(f"  <10ç±³ç²¾åº¦: {accuracy_10m*100:.1f}%")
    print(f"  <50ç±³ç²¾åº¦: {accuracy_50m*100:.1f}%")
    print(f"  <100ç±³ç²¾åº¦: {accuracy_100m*100:.1f}%")
    
    return metrics

if __name__ == '__main__':
    # æµ‹è¯•æœ€æ–°æ¨¡å‹
    checkpoint = "checkpoints_real/20251206_124644/final_model.pth"
    if os.path.exists(checkpoint):
        metrics = test_model_correctly(checkpoint, num_samples=50)
        
        # ä¸Baselineæ¯”è¾ƒ
        print(f"\nğŸ“ˆ ä¸DRL Baselineå¯¹æ¯”:")
        print(f"  æˆ‘ä»¬çš„æ¨¡å‹: {metrics['avg_distance_m']:.1f}ç±³")
        print(f"  DRL Baseline: 25.3ç±³ (è®ºæ–‡æŠ¥å‘Š)")
        
        if metrics['avg_distance_m'] < 25.3:
            improvement = (25.3 - metrics['avg_distance_m']) / 25.3 * 100
            print(f"  âœ… ä¼˜äºBaseline: æå‡ {improvement:.1f}%")
        else:
            improvement = (metrics['avg_distance_m'] - 25.3) / 25.3 * 100
            print(f"  âš ï¸  å·®äºBaseline: å·® {improvement:.1f}%")
            
    else:
        print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint}")