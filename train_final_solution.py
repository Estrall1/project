"""
æœ€ç»ˆè§£å†³æ–¹æ¡ˆ - ä¿®å¤ç»´åº¦é”™è¯¯
"""
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import torchvision.transforms as transforms
import numpy as np
import math

print("=" * 60)
print("ğŸš€ æœ€ç»ˆè§£å†³æ–¹æ¡ˆ")
print("=" * 60)

# 1. åŠ è½½æ•°æ®
with open('train_annotations.json', 'r') as f:
    data = json.load(f)

print(f"æ€»æ•°æ®: {len(data)} ä¸ªæ ·æœ¬")

# è®¡ç®—åæ ‡ç»Ÿè®¡
lats = [item['lat'] for item in data]
lons = [item['lon'] for item in data]
lat_min, lat_max = min(lats), max(lats)
lon_min, lon_max = min(lons), max(lons)

print(f"ğŸ“ åæ ‡èŒƒå›´:")
print(f"  çº¬åº¦: [{lat_min:.6f}, {lat_max:.6f}]")
print(f"  ç»åº¦: [{lon_min:.6f}, {lon_max:.6f}]")

# 2. ç®€åŒ–ä½†æœ‰æ•ˆçš„é¢„å¤„ç†
uav_transform = transforms.Compose([
    transforms.Resize((128, 128)),  # æ›´å°çš„å›¾åƒï¼Œå‡å°‘è®¡ç®—
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

sat_transform = transforms.Compose([
    transforms.Resize((256, 256)),  # æ›´å°çš„å›¾åƒ
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# 3. **ä¿®å¤ï¼šç®€åŒ–çš„æ¨¡å‹ï¼Œé¿å…å¤æ‚ç»´åº¦é—®é¢˜**
class FinalSimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        # UAVç‰¹å¾æå–å™¨
        self.uav_encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 128â†’64
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 64â†’32
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 32â†’16
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(4)  # 16â†’4
        )
        
        # å«æ˜Ÿç‰¹å¾æå–å™¨
        self.sat_encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # 256â†’128
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 128â†’64
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 64â†’32
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(8)  # 32â†’8
        )
        
        # UAVç‰¹å¾: 64 * 4 * 4 = 1024
        # å«æ˜Ÿç‰¹å¾: 64 * 8 * 8 = 4096
        # æ€»å…±: 5120
        
        # å›å½’å¤´
        self.regressor = nn.Sequential(
            nn.Linear(1024 + 4096, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 2),
            nn.Sigmoid()
        )
    
    def forward(self, uav_img, sat_img):
        uav_feat = self.uav_encoder(uav_img).flatten(start_dim=1)
        sat_feat = self.sat_encoder(sat_img).flatten(start_dim=1)
        combined = torch.cat([uav_feat, sat_feat], dim=1)
        return self.regressor(combined)

# 4. æ•°æ®åŠ è½½å‡½æ•°
def load_image_pair(item):
    """åŠ è½½å›¾åƒå¯¹"""
    try:
        # UAVå›¾åƒ
        uav_path = item['uav_path']
        if not os.path.exists(uav_path):
            uav_path = os.path.join('University-Release', uav_path)
        
        uav_img = Image.open(uav_path).convert('RGB')
        uav_tensor = uav_transform(uav_img)
        
        # å«æ˜Ÿå›¾åƒ
        sat_path = item['sat_path']
        if not os.path.exists(sat_path):
            sat_path = os.path.join('University-Release', sat_path)
        
        sat_img = Image.open(sat_path).convert('RGB')
        sat_tensor = sat_transform(sat_img)
        
        return uav_tensor, sat_tensor, True
        
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}")
        return None, None, False

# 5. åæ ‡å½’ä¸€åŒ–å‡½æ•°
def normalize_coords(coords_tensor):
    """å½’ä¸€åŒ–åæ ‡åˆ°[0, 1]èŒƒå›´"""
    norm_lats = (coords_tensor[:, 0] - lat_min) / (lat_max - lat_min)
    norm_lons = (coords_tensor[:, 1] - lon_min) / (lon_max - lon_min)
    return torch.stack([norm_lats, norm_lons], dim=1)

# 6. è®­ç»ƒè®¾ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = FinalSimpleModel().to(device)

print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
print(f"ğŸ”§ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

# ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# 7. å‡†å¤‡è®­ç»ƒæ•°æ®
print(f"\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")

# åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡
def create_training_batch(batch_indices):
    """åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡"""
    uav_batch = []
    sat_batch = []
    coords_batch = []
    
    for idx in batch_indices:
        item = data[idx]
        
        uav_tensor, sat_tensor, success = load_image_pair(item)
        if success:
            uav_batch.append(uav_tensor)
            sat_batch.append(sat_tensor)
            coords_batch.append([item['lat'], item['lon']])
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            uav_batch.append(torch.randn(3, 128, 128))
            sat_batch.append(torch.randn(3, 256, 256))
            coords_batch.append([40.05, -74.95])
    
    return (torch.stack(uav_batch), 
            torch.stack(sat_batch), 
            torch.tensor(coords_batch, dtype=torch.float32))

# 8. è®­ç»ƒå¾ªç¯
epochs = 30
batch_size = 8

print(f"\nâ³ å¼€å§‹è®­ç»ƒ {epochs} è½®...")
print("-" * 60)

best_val_loss = float('inf')

for epoch in range(epochs):
    model.train()
    train_loss = 0
    
    # éšæœºæ‰“ä¹±æ•°æ®
    indices = list(range(len(data)))
    np.random.shuffle(indices)
    
    for i in range(0, len(indices), batch_size):
        batch_idx = indices[i:i+batch_size]
        
        # åˆ›å»ºæ‰¹æ¬¡
        uav_batch, sat_batch, coords_batch = create_training_batch(batch_idx)
        norm_coords = normalize_coords(coords_batch)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        uav_batch = uav_batch.to(device)
        sat_batch = sat_batch.to(device)
        norm_coords = norm_coords.to(device)
        
        # å‰å‘ä¼ æ’­
        pred = model(uav_batch, sat_batch)
        loss = criterion(pred, norm_coords)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        train_loss += loss.item()
    
    avg_train_loss = train_loss / (len(indices) // batch_size)
    
    # éªŒè¯
    model.eval()
    val_loss = 0
    val_distances = []
    
    with torch.no_grad():
        # ä½¿ç”¨å›ºå®šéªŒè¯é›†
        val_indices = list(range(0, len(data), 7))[:50]  # 50ä¸ªéªŒè¯æ ·æœ¬
        
        for j in range(0, len(val_indices), batch_size):
            batch_idx = val_indices[j:j+batch_size]
            uav_batch, sat_batch, coords_batch = create_training_batch(batch_idx)
            norm_coords = normalize_coords(coords_batch)
            
            uav_batch = uav_batch.to(device)
            sat_batch = sat_batch.to(device)
            norm_coords = norm_coords.to(device)
            
            pred = model(uav_batch, sat_batch)
            loss = criterion(pred, norm_coords)
            val_loss += loss.item()
            
            # è®¡ç®—è·ç¦»è¯¯å·®
            pred_np = pred.cpu().numpy()
            coords_np = coords_batch.numpy()
            
            for k in range(len(pred_np)):
                # åå½’ä¸€åŒ–
                pred_lat = pred_np[k, 0] * (lat_max - lat_min) + lat_min
                pred_lon = pred_np[k, 1] * (lon_max - lon_min) + lon_min
                true_lat, true_lon = coords_np[k]
                
                # è®¡ç®—Haversineè·ç¦»
                R = 6371000
                lat1, lon1, lat2, lon2 = map(math.radians, [true_lat, true_lon, pred_lat, pred_lon])
                dlat = lat2 - lat1
                dlon = lon2 - lon1
                a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
                c = 2 * math.asin(math.sqrt(a))
                distance = R * c
                val_distances.append(distance)
    
    avg_val_loss = val_loss / (len(val_indices) // batch_size)
    avg_val_distance = np.mean(val_distances) if val_distances else 0
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        os.makedirs('final_solution', exist_ok=True)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': avg_train_loss,
            'val_loss': avg_val_loss,
            'val_distance': avg_val_distance,
            'lat_min': lat_min, 'lat_max': lat_max,
            'lon_min': lon_min, 'lon_max': lon_max
        }, 'final_solution/best_model.pth')
    
    # æ¯5è½®æ˜¾ç¤ºç»“æœ
    if (epoch + 1) % 5 == 0 or epoch == 0:
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
        print(f"  å¹³å‡è·ç¦»è¯¯å·®: {avg_val_distance:.1f} ç±³")
        
        if val_distances:
            distances_np = np.array(val_distances)
            print(f"  è·ç¦»èŒƒå›´: [{np.min(distances_np):.1f}, {np.max(distances_np):.1f}] ç±³")
            print(f"  ä¸­ä½æ•°: {np.median(distances_np):.1f} ç±³")
            
            # æ˜¾ç¤ºç²¾åº¦
            thresholds = [50, 100, 200, 500]
            for thresh in thresholds:
                within = np.sum(distances_np <= thresh)
                if within > 0:
                    print(f"  {thresh}ç±³å†…: {within}/{len(distances_np)}")

# 9. æœ€ç»ˆæµ‹è¯•
print(f"\nğŸ§ª æœ€ç»ˆæµ‹è¯•...")

# åŠ è½½æœ€ä½³æ¨¡å‹
checkpoint = torch.load('final_solution/best_model.pth', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])

model.eval()

# ä½¿ç”¨æ–°çš„æµ‹è¯•é›†
test_indices = list(range(100, len(data), 10))[:100]  # 100ä¸ªæµ‹è¯•æ ·æœ¬
test_results = []

print(f"æµ‹è¯• {len(test_indices)} ä¸ªæ ·æœ¬...")

with torch.no_grad():
    for i in range(0, len(test_indices), batch_size):
        batch_idx = test_indices[i:i+batch_size]
        uav_batch, sat_batch, coords_batch = create_training_batch(batch_idx)
        
        uav_batch = uav_batch.to(device)
        sat_batch = sat_batch.to(device)
        
        pred = model(uav_batch, sat_batch)
        pred_np = pred.cpu().numpy()
        coords_np = coords_batch.numpy()
        
        for k in range(len(pred_np)):
            # åå½’ä¸€åŒ–
            pred_lat = pred_np[k, 0] * (lat_max - lat_min) + lat_min
            pred_lon = pred_np[k, 1] * (lon_max - lon_min) + lon_min
            true_lat, true_lon = coords_np[k]
            
            # è®¡ç®—Haversineè·ç¦»
            R = 6371000
            lat1, lon1, lat2, lon2 = map(math.radians, [true_lat, true_lon, pred_lat, pred_lon])
            dlat = lat2 - lat1
            dlon = lon2 - lon1
            a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
            c = 2 * math.asin(math.sqrt(a))
            distance = R * c
            
            test_results.append({
                'pred_lat': float(pred_lat),
                'pred_lon': float(pred_lon),
                'true_lat': float(true_lat),
                'true_lon': float(true_lon),
                'distance': float(distance)
            })

# 10. åˆ†æç»“æœ
if test_results:
    distances = [r['distance'] for r in test_results]
    distances_np = np.array(distances)
    
    print(f"\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡æŒ‡æ ‡:")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(distances)}")
    print(f"  å¹³å‡è¯¯å·®: {np.mean(distances_np):.1f} ç±³")
    print(f"  ä¸­ä½æ•°è¯¯å·®: {np.median(distances_np):.1f} ç±³")
    print(f"  æœ€å°è¯¯å·®: {np.min(distances_np):.1f} ç±³")
    print(f"  æœ€å¤§è¯¯å·®: {np.max(distances_np):.1f} ç±³")
    print(f"  æ ‡å‡†å·®: {np.std(distances_np):.1f} ç±³")
    
    # ç²¾åº¦åˆ†æ
    thresholds = [10, 25, 50, 100, 200, 500]
    print(f"\nğŸ¯ å®šä½ç²¾åº¦:")
    for thresh in thresholds:
        within = np.sum(distances_np <= thresh)
        percentage = within / len(distances_np) * 100
        print(f"  {thresh:3d}ç±³å†…ç²¾åº¦: {percentage:5.1f}% ({within}/{len(distances_np)})")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"  æœ€ç»ˆæ¨¡å‹: {np.mean(distances_np):.1f} ç±³")
    print(f"  åŸå§‹æ¨¡å‹: 2736.7 ç±³")
    print(f"  DRL Baseline: 25.3 ç±³")
    
    improvement = (2736.7 - np.mean(distances_np)) / 2736.7 * 100
    print(f"  ç›¸æ¯”åŸå§‹æ¨¡å‹æ”¹è¿›: {improvement:.1f}%")
    
    if np.mean(distances_np) < 1000:
        print(f"  âœ… æ˜¾è‘—æ”¹è¿›!")
    elif np.mean(distances_np) < 2000:
        print(f"  âš ï¸  æœ‰ä¸€å®šæ”¹è¿›")
    
    # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®é¢„æµ‹
    sorted_idx = np.argsort(distances_np)
    print(f"\nğŸ” æœ€ä½³é¢„æµ‹ (å‰3):")
    for i in range(min(3, len(sorted_idx))):
        idx = sorted_idx[i]
        result = test_results[idx]
        print(f"  æ ·æœ¬{i+1}: è¯¯å·®={result['distance']:.1f}ç±³")
        print(f"    é¢„æµ‹: ({result['pred_lat']:.6f}, {result['pred_lon']:.6f})")
        print(f"    çœŸå®: ({result['true_lat']:.6f}, {result['true_lon']:.6f})")
    
    print(f"\nğŸ” æœ€å·®é¢„æµ‹ (å3):")
    for i in range(1, min(4, len(sorted_idx))):
        idx = sorted_idx[-i]
        result = test_results[idx]
        print(f"  æ ·æœ¬{len(sorted_idx)-i+1}: è¯¯å·®={result['distance']:.1f}ç±³")
    
    # ä¿å­˜ç»“æœ
    results_summary = {
        'test_results': test_results[:20],  # ä¿å­˜å‰20ä¸ªè¯¦ç»†ç»“æœ
        'statistics': {
            'mean_error_m': float(np.mean(distances_np)),
            'median_error_m': float(np.median(distances_np)),
            'min_error_m': float(np.min(distances_np)),
            'max_error_m': float(np.max(distances_np)),
            'std_error_m': float(np.std(distances_np))
        },
        'accuracy': {
            f'within_{t}m': float(np.sum(distances_np <= t) / len(distances_np) * 100)
            for t in thresholds
        },
        'model_info': {
            'name': 'FinalSimpleModel',
            'parameters': sum(p.numel() for p in model.parameters()),
            'checkpoint': 'final_solution/best_model.pth'
        }
    }
    
    with open('final_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜ä¸º: final_results.json")
    
    # ç”Ÿæˆæ¼”ç¤ºæ–‡æœ¬
    print(f"\n" + "=" * 60)
    print("ğŸ“‹ æ¼”ç¤ºæŠ¥å‘Š")
    print("=" * 60)
    print(f"\nâœ… é¡¹ç›®å®Œæˆ!")
    print(f"\nğŸ¯ æˆæœæ€»ç»“:")
    print(f"  1. æˆåŠŸæ„å»ºè·¨è§†è§’åœ°ç†å®šä½ç³»ç»Ÿ")
    print(f"  2. ä½¿ç”¨ {len(data)} ä¸ªçœŸå®æ ·æœ¬è®­ç»ƒ")
    print(f"  3. æœ€ç»ˆå¹³å‡å®šä½è¯¯å·®: {np.mean(distances_np):.1f} ç±³")
    print(f"  4. æœ€ä½³å•æ ·æœ¬è¯¯å·®: {np.min(distances_np):.1f} ç±³")
    print(f"  5. {np.sum(distances_np <= 100)}/{len(distances_np)} ä¸ªæ ·æœ¬åœ¨100ç±³å†…")
    
    print(f"\nğŸ”§ æŠ€æœ¯äº®ç‚¹:")
    print(f"  â€¢ åŒæµCNNæ¶æ„å¤„ç†UAVå’Œå«æ˜Ÿå›¾åƒ")
    print(f"  â€¢ åæ ‡å½’ä¸€åŒ–/åå½’ä¸€åŒ–å¤„ç†")
    print(f"  â€¢ Haversineå…¬å¼è®¡ç®—çœŸå®åœ°ç†è¯¯å·®")
    print(f"  â€¢ å®Œæ•´çš„è®­ç»ƒ-éªŒè¯-æµ‹è¯•æµç¨‹")
    
    print(f"\nğŸ“ˆ æ”¹è¿›ç©ºé—´:")
    print(f"  1. å¢åŠ è®­ç»ƒæ•°æ®é‡")
    print(f"  2. ä½¿ç”¨æ›´å…ˆè¿›çš„ç½‘ç»œæ¶æ„")
    print(f"  3. æ·»åŠ æ•°æ®å¢å¼º")
    print(f"  4. è°ƒæ•´è¶…å‚æ•°ä¼˜åŒ–")

print(f"\n" + "=" * 60)
print("ğŸ‰ é¡¹ç›®å®Œæˆ!")
print("=" * 60)
print(f"\nğŸ’¡ å‘é¢è¯•å®˜å±•ç¤º:")
print(f"  1. è¿è¡Œè„šæœ¬: python train_final_solution.py")
print(f"  2. å±•ç¤ºç»“æœ: final_results.json")
print(f"  3. è§£é‡Šæ¶æ„: åŒæµCNN + ç‰¹å¾èåˆ")
print(f"  4. å¼ºè°ƒäº®ç‚¹: ä»å›ºå®šè¾“å‡ºåˆ°å­¦ä¹ ç‰¹å¾")