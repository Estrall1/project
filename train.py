# train_real_fixed.py
"""
ä¿®å¤ç‰ˆè®­ç»ƒä»£ç  - ç¡®ä¿èƒ½åŠ è½½å®Œæ•´æ•°æ®é›†
"""
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import argparse
import logging
from tqdm import tqdm
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import math
import json
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

# å¯¼å…¥
try:
    from dataset.crossview_real_dataset import RealUniversityDataset
    from models.crossview_model import AdvancedCrossViewGeolocator
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# è®¾ç½®matplotlibå­—ä½“ - è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.family'] = 'DejaVu Sans'  # ä½¿ç”¨è‹±æ–‡å­—ä½“é¿å…ä¹±ç 
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def haversine_distance(pred_coords, true_coords, R=6371000.0):
    """è®¡ç®—Haversineè·ç¦»"""
    pred_lat = torch.deg2rad(pred_coords[:, 0])
    pred_lon = torch.deg2rad(pred_coords[:, 1])
    true_lat = torch.deg2rad(true_coords[:, 0])
    true_lon = torch.deg2rad(true_coords[:, 1])
    
    dlat = true_lat - pred_lat
    dlon = true_lon - pred_lon
    
    a = torch.sin(dlat/2)**2 + torch.cos(pred_lat) * torch.cos(true_lat) * torch.sin(dlon/2)**2
    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))
    distance = R * c
    
    return distance

class RealGeolocationLoss(nn.Module):
    """çœŸå®åœ°ç†å®šä½æŸå¤±"""
    def __init__(self, lambda_coord=1.0):
        super().__init__()
        self.mse = nn.MSELoss()
        self.huber = nn.SmoothL1Loss()
        self.lambda_coord = lambda_coord
        
    def forward(self, outputs, labels):
        losses = {}
        
        if 'fine_coords' in outputs and 'latlon' in labels:
            # MSEæŸå¤±
            mse_loss = self.mse(outputs['fine_coords'], labels['latlon'])
            
            # HuberæŸå¤±ï¼ˆæ›´é²æ£’ï¼‰
            huber_loss = self.huber(outputs['fine_coords'], labels['latlon'])
            
            # Haversineè·ç¦»ï¼ˆç”¨äºç›‘æ§ï¼‰
            with torch.no_grad():
                haversine_dist = haversine_distance(outputs['fine_coords'], labels['latlon'])
                losses['haversine'] = haversine_dist.mean()
            
            # ç»„åˆæŸå¤±
            total_loss = self.lambda_coord * (mse_loss + huber_loss)
            losses['total'] = total_loss
            losses['mse'] = mse_loss
            losses['huber'] = huber_loss
        
        return losses

def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    haversine_distances = []
    
    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1} Training')
    for batch_idx, batch in enumerate(progress_bar):
        try:
            # è·å–æ•°æ® - æ·»åŠ è°ƒè¯•ä¿¡æ¯
            uav_imgs = batch['uav'].to(device)
            satellite_imgs = batch['satellite'].to(device)
            lat_labels = batch['lat'].to(device)
            lon_labels = batch['lon'].to(device)
            
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            B = uav_imgs.shape[0]
            if B == 0:
                logger.warning(f"æ‰¹æ¬¡ {batch_idx} ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # å‡†å¤‡æ ‡ç­¾
            latlon_labels = torch.cat([lat_labels.view(B, 1), lon_labels.view(B, 1)], dim=1)
            
            # æ£€æŸ¥æ ‡ç­¾èŒƒå›´ï¼ˆåº”è¯¥æ˜¯0-1çš„å½’ä¸€åŒ–å€¼ï¼‰
            if torch.any(latlon_labels < 0) or torch.any(latlon_labels > 1):
                logger.warning(f"æ‰¹æ¬¡ {batch_idx}: æ ‡ç­¾è¶…å‡º[0,1]èŒƒå›´")
            
            # å‰å‘ä¼ æ’­
            outputs = model(uav_imgs, satellite_imgs)
            
            # æ£€æŸ¥è¾“å‡º
            if 'fine_coords' not in outputs:
                logger.error(f"æ¨¡å‹æ²¡æœ‰è¾“å‡º'fine_coords'")
                continue
            
            # è®¡ç®—æŸå¤±
            labels = {'latlon': latlon_labels}
            losses = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            losses['total'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # è®°å½•æŸå¤±
            total_loss += losses['total'].item()
            
            # è®°å½•Haversineè·ç¦»
            if 'haversine' in losses:
                haversine_distances.append(losses['haversine'].item())
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_dist = np.mean(haversine_distances[-10:]) if haversine_distances else 0
            progress_bar.set_postfix({
                'loss': f"{losses['total'].item():.4f}",
                'dist': f"{avg_dist:.1f}m"
            })
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
    
    num_batches = len(dataloader)
    metrics = {
        'total_loss': total_loss / num_batches if num_batches > 0 else 0,
        'avg_distance': np.mean(haversine_distances) if haversine_distances else 0,
    }
    
    return metrics

def validate(model, dataloader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0.0
    haversine_distances = []
    
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc='Validation')
        for batch_idx, batch in enumerate(progress_bar):
            try:
                uav_imgs = batch['uav'].to(device)
                satellite_imgs = batch['satellite'].to(device)
                lat_labels = batch['lat'].to(device)
                lon_labels = batch['lon'].to(device)
                
                B = uav_imgs.shape[0]
                if B == 0:
                    continue
                
                latlon_labels = torch.cat([lat_labels.view(B, 1), lon_labels.view(B, 1)], dim=1)
                
                outputs = model(uav_imgs, satellite_imgs)
                labels = {'latlon': latlon_labels}
                losses = criterion(outputs, labels)
                
                total_loss += losses['total'].item()
                
                if 'haversine' in losses:
                    haversine_distances.append(losses['haversine'].item())
                
                avg_dist = np.mean(haversine_distances[-10:]) if haversine_distances else 0
                progress_bar.set_postfix({
                    'loss': f"{losses['total'].item():.4f}",
                    'dist': f"{avg_dist:.1f}m"
                })
                
            except Exception as e:
                logger.error(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
    
    num_batches = len(dataloader)
    if num_batches == 0:
        return {
            'total_loss': 0,
            'avg_distance': 0,
            'median_distance': 0,
            'min_distance': 0,
            'max_distance': 0,
        }
    
    metrics = {
        'total_loss': total_loss / num_batches,
        'avg_distance': np.mean(haversine_distances) if haversine_distances else 0,
        'median_distance': np.median(haversine_distances) if haversine_distances else 0,
        'min_distance': np.min(haversine_distances) if haversine_distances else 0,
        'max_distance': np.max(haversine_distances) if haversine_distances else 0,
    }
    
    return metrics

def check_data_samples(data_root='University-Release'):
    """æ£€æŸ¥æ•°æ®é›†æ ·æœ¬æ•°é‡"""
    try:
        # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶
        train_json = os.path.join(data_root, 'train', 'train_annotations.json')
        test_json = os.path.join(data_root, 'test', 'test_annotations.json')
        
        if os.path.exists(train_json):
            with open(train_json, 'r') as f:
                train_data = json.load(f)
            logger.info(f"è®­ç»ƒæ ‡ç­¾æ–‡ä»¶: {len(train_data)} ä¸ªæ ·æœ¬")
        
        if os.path.exists(test_json):
            with open(test_json, 'r') as f:
                test_data = json.load(f)
            logger.info(f"æµ‹è¯•æ ‡ç­¾æ–‡ä»¶: {len(test_data)} ä¸ªæ ·æœ¬")
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶
        uav_train_dir = os.path.join(data_root, 'train', 'uav')
        sat_train_dir = os.path.join(data_root, 'train', 'satellite')
        
        if os.path.exists(uav_train_dir):
            uav_files = [f for f in os.listdir(uav_train_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
            logger.info(f"UAVè®­ç»ƒå›¾åƒ: {len(uav_files)} ä¸ªæ–‡ä»¶")
        
        if os.path.exists(sat_train_dir):
            sat_files = [f for f in os.listdir(sat_train_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
            logger.info(f"å«æ˜Ÿè®­ç»ƒå›¾åƒ: {len(sat_files)} ä¸ªæ–‡ä»¶")
            
    except Exception as e:
        logger.warning(f"æ£€æŸ¥æ•°æ®é›†æ—¶å‡ºé”™: {e}")

def main():
    parser = argparse.ArgumentParser(description='Train with Real Geolocation Labels - Fixed Version')
    parser.add_argument('--data_root', type=str, default='University-Release', 
                       help='æ•°æ®é›†æ ¹ç›®å½•ï¼ŒåŒ…å«train/testå­ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gpu', type=int, default=0, help='GPUè®¾å¤‡')
    parser.add_argument('--train_samples', type=int, default=-1, 
                       help='è®­ç»ƒæ ·æœ¬æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨')
    parser.add_argument('--val_samples', type=int, default=100, 
                       help='éªŒè¯æ ·æœ¬æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨')
    parser.add_argument('--num_workers', type=int, default=2, 
                       help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨å¤šçº¿ç¨‹')
    parser.add_argument('--save_every', type=int, default=5, 
                       help='æ¯å¤šå°‘epochä¿å­˜ä¸€æ¬¡æ¨¡å‹')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®é›†
    logger.info(f"æ•°æ®é›†æ ¹ç›®å½•: {args.data_root}")
    if not os.path.exists(args.data_root):
        logger.error(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {args.data_root}")
        logger.info("è¯·ç¡®ä¿æ•°æ®é›†å·²ä¸‹è½½å¹¶è§£å‹åˆ°æ­£ç¡®ä½ç½®")
        logger.info("æ•°æ®é›†åº”è¯¥åŒ…å« train/ å’Œ test/ ç›®å½•")
        return
    
    check_data_samples(args.data_root)
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available() and args.gpu >= 0:
        device = torch.device(f'cuda:{args.gpu}')
        logger.info(f'ä½¿ç”¨GPU: {torch.cuda.get_device_name(args.gpu)}')
    else:
        device = torch.device('cpu')
        logger.info('ä½¿ç”¨CPU')
    
    # åˆ›å»ºæ•°æ®é›† - æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
    logger.info("åŠ è½½çœŸå®æ ‡ç­¾æ•°æ®é›†...")
    try:
        train_dataset = RealUniversityDataset(
            root_dir=args.data_root,
            split='train',
            uav_size=(256, 256),
            sat_size=(512, 512),
            max_samples=args.train_samples if args.train_samples > 0 else None
        )
        
        val_dataset = RealUniversityDataset(
            root_dir=args.data_root,
            split='test',
            uav_size=(256, 256),
            sat_size=(512, 512),
            max_samples=args.val_samples if args.val_samples > 0 else None
        )
        
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ:")
        logger.info(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        logger.info(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        
        if len(train_dataset) > 0:
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä¿¡æ¯
            sample = train_dataset[0]
            logger.info(f"   æ•°æ®å½¢çŠ¶: UAV={sample['uav'].shape}, Satellite={sample['satellite'].shape}")
            logger.info(f"   åæ ‡ç¤ºä¾‹: lat={sample['lat'].item():.6f}, lon={sample['lon'].item():.6f}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=args.batch_size, 
            shuffle=True, 
            num_workers=min(2, args.num_workers),  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=args.batch_size,
            shuffle=False, 
            num_workers=min(2, args.num_workers),
            pin_memory=True if torch.cuda.is_available() else False
        )
        
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å°è¯•ä½¿ç”¨å¤‡ç”¨æ•°æ®é›†
        logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨æ•°æ®é›†è·¯å¾„...")
        alt_paths = [
            '.',
            './data',
            '../University-Release',
            '../../University-Release'
        ]
        
        for alt_path in alt_paths:
            if os.path.exists(os.path.join(alt_path, 'train_annotations.json')):
                logger.info(f"æ‰¾åˆ°å¤‡ç”¨è·¯å¾„: {alt_path}")
                args.data_root = alt_path
                break
        
        # é‡æ–°å°è¯•
        try:
            train_dataset = RealUniversityDataset(
                root_dir=args.data_root,
                split='train',
                uav_size=(256, 256),
                sat_size=(512, 512),
                max_samples=args.train_samples if args.train_samples > 0 else None
            )
            val_dataset = RealUniversityDataset(
                root_dir=args.data_root,
                split='test',
                uav_size=(256, 256),
                sat_size=(512, 512),
                max_samples=args.val_samples if args.val_samples > 0 else None
            )
            logger.info("âœ… å¤‡ç”¨è·¯å¾„åŠ è½½æˆåŠŸ")
        except:
            logger.error("æ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†")
            return
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("åˆå§‹åŒ–æ¨¡å‹...")
    try:
        model = AdvancedCrossViewGeolocator().to(device)
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    except Exception as e:
        logger.error(f"åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
        # ä½¿ç”¨ç®€åŒ–æ¨¡å‹ä½œä¸ºåå¤‡
        logger.info("ä½¿ç”¨ç®€åŒ–æ¨¡å‹ä½œä¸ºåå¤‡...")
        from models.simple_model import SimpleCrossViewModel
        model = SimpleCrossViewModel().to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = RealGeolocationLoss(lambda_coord=1.0)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = os.path.join('checkpoints_real', timestamp)
    os.makedirs(save_dir, exist_ok=True)
    logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {save_dir}")
    
    # ä¿å­˜é…ç½®
    config_file = os.path.join(save_dir, 'config.txt')
    with open(config_file, 'w') as f:
        f.write(f"è®­ç»ƒæ—¶é—´: {timestamp}\n")
        f.write(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}\n")
        f.write(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}\n")
        f.write(f"éªŒè¯æ ·æœ¬: {len(val_dataset)}\n")
        f.write(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
        f.write(f"è®­ç»ƒè½®æ•°: {args.epochs}\n")
        f.write(f"å­¦ä¹ ç‡: {args.lr}\n")
        f.write(f"è®¾å¤‡: {device}\n")
    
    # è®­ç»ƒå¾ªç¯
    best_val_distance = float('inf')
    train_losses = []
    val_losses = []
    val_distances = []
    
    logger.info("å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(args.epochs):
        logger.info(f'\n{"="*60}')
        logger.info(f'Epoch {epoch+1}/{args.epochs}')
        logger.info(f'å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.6f}')
        
        # è®­ç»ƒ
        train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)
        train_losses.append(train_metrics['total_loss'])
        
        logger.info(f'è®­ç»ƒæŸå¤±: {train_metrics["total_loss"]:.6f}')
        logger.info(f'å¹³å‡è·ç¦»: {train_metrics["avg_distance"]:.1f}ç±³')
        
        # éªŒè¯
        val_metrics = validate(model, val_loader, criterion, device)
        val_losses.append(val_metrics['total_loss'])
        val_distances.append([
            val_metrics['avg_distance'],
            val_metrics['median_distance'],
            val_metrics['min_distance'],
            val_metrics['max_distance']
        ])
        
        logger.info(f'éªŒè¯æŸå¤±: {val_metrics["total_loss"]:.6f}')
        logger.info(f'å¹³å‡è·ç¦»: {val_metrics["avg_distance"]:.1f}ç±³')
        logger.info(f'ä¸­ä½æ•°è·ç¦»: {val_metrics["median_distance"]:.1f}ç±³')
        logger.info(f'èŒƒå›´: [{val_metrics["min_distance"]:.1f}, {val_metrics["max_distance"]:.1f}]ç±³')
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if (epoch + 1) % args.save_every == 0:
            model_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_metrics['total_loss'],
                'val_loss': val_metrics['total_loss'],
                'val_distance': val_metrics['avg_distance'],
            }, model_path)
            logger.info(f'ğŸ’¾ ä¿å­˜ä¸­é—´æ¨¡å‹: {model_path}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['avg_distance'] < best_val_distance:
            best_val_distance = val_metrics['avg_distance']
            model_path = os.path.join(save_dir, f'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_metrics['total_loss'],
                'val_loss': val_metrics['total_loss'],
                'val_distance': val_metrics['avg_distance'],
                'best_val_distance': best_val_distance,
            }, model_path)
            logger.info(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}')
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(save_dir, 'final_model.pth')
    torch.save({
        'epoch': args.epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_distances': val_distances,
        'best_val_distance': best_val_distance,
        'args': vars(args)
    }, final_path)
    logger.info(f'ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}')
    
    # ç»˜åˆ¶ç»“æœ - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…ä¹±ç 
    try:
        results_dir = 'results_real'
        os.makedirs(results_dir, exist_ok=True)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # æŸå¤±æ›²çº¿
        epochs_range = range(1, len(train_losses) + 1)
        ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss', marker='o', markersize=4)
        ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss', marker='s', markersize=4)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title(f'Training Progress\nBest Validation Distance: {best_val_distance:.1f}m')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # è·ç¦»è¯¯å·®
        ax2.plot(epochs_range, [d[0] for d in val_distances], 'g-', label='Avg Distance', marker='^', markersize=4)
        ax2.plot(epochs_range, [d[1] for d in val_distances], 'm-', label='Median Distance', marker='d', markersize=4)
        ax2.axhline(y=25.3, color='r', linestyle='--', alpha=0.7, label='Baseline (25.3m)')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Distance Error (m)')
        ax2.set_title('Validation Distance Error')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, f'training_results_{timestamp}.png'), 
                   dpi=150, bbox_inches='tight')
        
        # å•ç‹¬ä¿å­˜ä¸€ä¸ªç®€å•çš„æŸå¤±æ›²çº¿
        plt.figure(figsize=(8, 5))
        plt.plot(train_losses, label='Training Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Loss Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(results_dir, 'loss_curve.png'), dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f'ğŸ“Š ç»“æœå›¾è¡¨ä¿å­˜åˆ°: {results_dir}/')
        
    except Exception as e:
        logger.warning(f"ç»˜åˆ¶å›¾è¡¨å¤±è´¥: {e}")
        import traceback
        logger.warning(traceback.format_exc())
    
    logger.info(f'\n{"="*60}')
    logger.info('ğŸ‰ è®­ç»ƒå®Œæˆ!')
    logger.info(f'è®­ç»ƒè½®æ•°: {args.epochs}')
    logger.info(f'æœ€ä½³éªŒè¯è·ç¦»: {best_val_distance:.1f}ç±³')
    logger.info(f'ä¸Baselineå¯¹æ¯”: {best_val_distance:.1f}m vs 25.3m')
    logger.info(f'æ¨¡å‹ä¿å­˜åˆ°: {save_dir}/')
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    if len(val_distances) > 0:
        final_avg = val_distances[-1][0]
        final_median = val_distances[-1][1]
        logger.info(f'æœ€ç»ˆéªŒè¯è·ç¦»: å¹³å‡={final_avg:.1f}m, ä¸­ä½æ•°={final_median:.1f}m')

if __name__ == '__main__':
    main()